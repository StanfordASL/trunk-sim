import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from typing import List, Tuple, Optional


class TrunkData:
    """
    Trunk data class to handle data generated by the Trunk simulator.
    
    Stores simulation data with time, states, and inputs in a pandas DataFrame.
    Provides functionality to save as CSV and convert to PyTorch dataset.
    """
    def __init__(self, states: str = "pos", links: List = [3]):
        """
        Initialize a TrunkData object.
        
        Args:
            states: states specification ("pos", "vel", "pos_vel")
            links: links to be included in the dataset, starting from 1
        """
        self.states = states
        self.links = links
        
        # Column name patterns
        self.time_col = "t"

        if self.states == "pos":
            self.state_cols = [f"{axis}{link}" for link in links for axis in ["x", "y", "z"]]
        elif self.states == "vel":
            self.state_cols = [f"{axis}{link}" for link in links for axis in ["vx", "vy", "vz"]]
        elif self.states == "pos_vel":
            self.state_cols = [f"{axis}{link}" for link in links for axis in ["x", "y", "z", "vx", "vy", "vz"]]
        else:
            raise ValueError(f"Invalid states specification: {self.states}")

        self.control_cols = [f"u{axis}{link}" for link in links for axis in ["x", "y"]]

        self.state_dim = len(self.state_cols)
        self.controls_dim = len(self.control_cols)
        
        # Initialize an empty DataFrame
        self.data = pd.DataFrame()
    
    def add_data(self, t: float, x: np.ndarray, u: np.ndarray) -> None:
        """
        Add a single data point to the dataset.
        
        Args:
            t: Time value
            x: State vector
            u: Input vector
        """
        # Check dimensions
        assert len(x) == self.state_dim, f"Expected state dimension {self.state_dim}, got {len(x)}"
        assert len(u) == self.controls_dim, f"Expected input dimension {self.controls_dim}, got {len(u)}"
            
        # Create a new row
        row_data = {self.time_col: t}
        
        # Add state and input data
        row_data.update(dict(zip(self.state_cols, x)))
        row_data.update(dict(zip(self.control_cols, u)))
            
        # Append the new row
        new_row = pd.DataFrame([row_data])
        self.data = pd.concat([self.data, new_row], ignore_index=True)
    
    def add_batch_data(self, t_batch: np.ndarray,
                      x_batch: np.ndarray,
                      u_batch: np.ndarray) -> None:
        """
        Add a batch of data points to the dataset.
        
        Args:
            t_batch: Array of time values of shape (batch_size,)
            x_batch: Array of state vectors of shape (batch_size, state_dim)
            u_batch: Array of input vectors of shape (batch_size, controls_dim)
        """
        t_batch = np.array(t_batch).flatten()
        x_batch = np.array(x_batch)
        u_batch = np.array(u_batch)
            
        # Create batch data
        batch_data = {self.time_col: t_batch}
        
        # Handle state and input shapes
        if len(x_batch.shape) == 1:
            x_batch = x_batch.reshape(1, -1)
        if len(u_batch.shape) == 1:
            u_batch = u_batch.reshape(1, -1)

        # Add state and input data            
        for i, col in enumerate(self.state_cols):
            batch_data[col] = x_batch[:, i]

        for i, col in enumerate(self.control_cols):
            batch_data[col] = u_batch[:, i]

        # Create and append the new batch
        batch_df = pd.DataFrame(batch_data)
        self.data = pd.concat([self.data, batch_df], ignore_index=True)
    
    def save_to_csv(self, filename: str) -> None:
        """
        Save the data to a CSV file.
        
        Args:
            filename: Path to the CSV file to save
        """
        self.data.to_csv(filename, index=False)
    
    def load_from_csv(self, filename: str) -> None:
        """
        Load data from a CSV file.
        
        Args:
            filename: Path to the CSV file to load
        """
        self.data = pd.read_csv(filename)
        
        # Infer column types
        cols = list(self.data.columns)
        if 't' in cols:
            self.time_col = 't'
            cols.remove('t')
        
        self.state_cols = [col for col in cols if col.startswith('x')]
        self.control_cols = [col for col in cols if col.startswith('u')]
        
        self.state_dim = len(self.state_cols)
        self.controls_dim = len(self.control_cols)
    
    def convert_to_torch_dataset(self, 
                                input_cols: Optional[List[str]] = None, 
                                output_cols: Optional[List[str]] = None) -> 'TrunkTorchDataset':
        """
        Convert to a PyTorch Dataset.
        
        Args:
            input_cols: List of column names to use as inputs (defaults to all state and input columns)
            output_cols: List of column names to use as outputs (defaults to all state columns)
            
        Returns:
            A TrunkTorchDataset instance
        """
        if input_cols is None:
            input_cols = self.state_cols
        if output_cols is None:
            output_cols = self.control_cols
            
        return TrunkTorchDataset(self.data, input_cols, output_cols)
    
    def get_state_at_time(self, t: float) -> np.ndarray:
        """
        Get the state vector at a specific time.
        
        Args:
            t: Time value
            
        Returns:
            State vector at time t
        """
        # Find the closest time
        closest_idx = (self.data[self.time_col] - t).abs().idxmin()
        return self.data.loc[closest_idx, self.state_cols].values
    
    def get_input_at_time(self, t: float) -> np.ndarray:
        """
        Get the input vector at a specific time.
        
        Args:
            t: Time value
            
        Returns:
            Input vector at time t
        """
        closest_idx = (self.data[self.time_col] - t).abs().idxmin()
        return self.data.loc[closest_idx, self.control_cols].values
    
    def __len__(self) -> int:
        """Return the number of data points."""
        return len(self.data)
    
    def __getitem__(self, idx):
        """Allow indexing the data directly."""
        return self.data.iloc[idx]


class TrunkTorchDataset(Dataset):
    """PyTorch Dataset wrapper for TrunkData."""
    
    def __init__(self, dataframe: pd.DataFrame, 
                 input_cols: List[str],
                 output_cols: List[str]):
        """
        Initialize a PyTorch dataset from a pandas DataFrame.
        
        Args:
            dataframe: Source DataFrame
            input_cols: Column names to use as inputs
            output_cols: Column names to use as outputs
        """
        self.dataframe = dataframe
        self.input_cols = input_cols
        self.output_cols = output_cols
    
    def __len__(self) -> int:
        """Return the number of data points."""
        return len(self.dataframe)
    
    def __getitem__(self, idx) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Get a single data point.
        
        Args:
            idx: Index of the data point
            
        Returns:
            Tuple of (input_tensor, output_tensor)
        """
        if isinstance(idx, torch.Tensor):
            idx = idx.item()
            
        # Get input and output values
        inputs = self.dataframe.iloc[idx][self.input_cols].values
        outputs = self.dataframe.iloc[idx][self.output_cols].values
        
        # Convert to PyTorch tensors
        input_tensor = torch.tensor(inputs, dtype=torch.float32)
        output_tensor = torch.tensor(outputs, dtype=torch.float32)
        
        return input_tensor, output_tensor
